{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bed199e0",
   "metadata": {},
   "source": [
    "The following code is taken from my original experimentation on Jupyter Notebook\n",
    "(It probably wont work as-is outside of that environment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1b093e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5160148a",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pandas'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpd\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mjson\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mre\u001b[39;00m\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'pandas'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import re\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "crimes_excel = pd.read_excel('/content/drive/MyDrive/Colab Notebooks/CivSentinel/Data/iowa_city_raw_6_29_2025.xlsx')\n",
    "with open('/content/drive/MyDrive/Colab Notebooks/CivSentinel/Data/iowa_city_location_mapping.json') as f:\n",
    "    crime_mapping_dict = json.load(f)\n",
    "\n",
    "cleaned_data = pd.DataFrame()\n",
    "DATE_REGEX = r'\\d{4}-\\d{2}-\\d{2} \\d{2}:\\d{2}:\\d{2}'\n",
    "\n",
    "for _, row in crimes_excel.iterrows():\n",
    "  if pd.isna(row['Associated ID']) or pd.isna(row['Date Reported']) or pd.isna(row['Date/Time Occurred']) or pd.isna(row['General Location']):\n",
    "    continue\n",
    "\n",
    "  date = re.search(DATE_REGEX, str(row['Date Reported']))\n",
    "\n",
    "  general_location = row['General Location'].lower().strip()\n",
    "\n",
    "  if general_location in crime_mapping_dict and pd.notna(crime_mapping_dict[general_location]) and date.group(0):\n",
    "    date_dt = datetime.strptime(date.group(0), '%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "    cleaned_data = pd.concat([cleaned_data, pd.DataFrame([{\n",
    "        'associated_id': row['Associated ID'],\n",
    "        'general_location': general_location,\n",
    "        'natures_of_crime': row['Nature of Crime(s)'],\n",
    "        'date': date_dt,\n",
    "        'latitude': crime_mapping_dict[general_location]['coordinates'][0],\n",
    "        'longitude': crime_mapping_dict[general_location]['coordinates'][1],\n",
    "    }])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9346ca34",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install h3\n",
    "\n",
    "import torch\n",
    "import h3\n",
    "from h3 import LatLngPoly\n",
    "\n",
    "coords_latitude = cleaned_data['latitude'].tolist()\n",
    "coords_longitude = cleaned_data['longitude'].tolist()\n",
    "\n",
    "max_lat, min_lat = max(coords_latitude), min(coords_latitude)\n",
    "max_lon, min_lon = max(coords_longitude), min(coords_longitude)\n",
    "\n",
    "polygon = LatLngPoly(\n",
    "  [\n",
    "    (min_lat, min_lon),\n",
    "    (min_lat, max_lon),\n",
    "    (max_lat, max_lon),\n",
    "    (max_lat, min_lon),\n",
    "    (min_lat, min_lon)\n",
    "  ]\n",
    ")\n",
    "\n",
    "all_hexes = h3.polygon_to_cells(polygon, 9)\n",
    "\n",
    "cleaned_data['h3_index'] = cleaned_data.apply(\n",
    "    lambda row: h3.latlng_to_cell(row['latitude'], row['longitude'], 9),\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "earliest_date = cleaned_data['date'].min()\n",
    "latest_date = cleaned_data['date'].max()\n",
    "\n",
    "full_range = pd.date_range(start=earliest_date, end=latest_date, freq=\"D\")\n",
    "\n",
    "crime_freq = cleaned_data.groupby(['h3_index', 'date']).size().unstack(fill_value=0)\n",
    "crime_freq = crime_freq.reindex(columns=full_range, fill_value=0)\n",
    "crime_freq = crime_freq.reindex(all_hexes, fill_value=0)\n",
    "\n",
    "hex_to_id = {}\n",
    "for i, hex in enumerate(all_hexes):\n",
    "  if hex not in hex_to_id:\n",
    "    hex_to_id[hex] = i\n",
    "\n",
    "graph_edges = []\n",
    "\n",
    "for hex in all_hexes:\n",
    "  neighbors = h3.grid_ring(hex, 1)\n",
    "  for neighbor in neighbors:\n",
    "    if neighbor in all_hexes:\n",
    "      graph_edges.append([hex_to_id[hex], hex_to_id[neighbor]])\n",
    "\n",
    "graph_edges = torch.tensor(graph_edges, dtype=torch.long).t().contiguous()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fa34b3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_rolling_windows(crime_freq, window_size=30, horizon=2, start_day=None, end_day=None):\n",
    "  if start_day is not None:\n",
    "    crime_freq = crime_freq.loc[:, start_day:]\n",
    "  if end_day is not None:\n",
    "    crime_freq = crime_freq.loc[:, :end_day]\n",
    "\n",
    "  windows = []\n",
    "  crime_array = crime_freq.values\n",
    "  num_days = crime_freq.shape[1]\n",
    "\n",
    "  for i in range(num_days - window_size - horizon + 1):\n",
    "    past_window = crime_array[:, i:i+window_size]\n",
    "    future_window = crime_array[:, i+window_size:i+window_size+horizon]\n",
    "    windows.append((past_window, future_window))\n",
    "\n",
    "  return windows\n",
    "\n",
    "train_set = create_rolling_windows(crime_freq, window_size=30, horizon=2, start_day='2023-01-01', end_day='2025-01-04')\n",
    "val_set = create_rolling_windows(crime_freq, window_size=30, horizon=2, start_day='2025-01-05', end_day='2025-04-02')\n",
    "test_set = create_rolling_windows(crime_freq, window_size=30, horizon=2, start_day='2025-04-03', end_day='2025-06-29')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6261803",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install torch-geometric\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torch_geometric.nn import GCNConv\n",
    "\n",
    "\n",
    "class STGNN(nn.Module):\n",
    "  def __init__(self, num_nodes, window_size, horizon, hidden_dim=64):\n",
    "    super(STGNN, self).__init__()\n",
    "    self.num_nodes = num_nodes\n",
    "    self.window_size = window_size\n",
    "    self.horizon = horizon\n",
    "    self.hidden_dim = hidden_dim\n",
    "\n",
    "    self.gru = nn.GRU(input_size=1, hidden_size=hidden_dim, batch_first=True)\n",
    "\n",
    "    self.gc1 = GCNConv(hidden_dim, hidden_dim)\n",
    "    self.gc2 = GCNConv(hidden_dim, hidden_dim)\n",
    "\n",
    "\n",
    "    self.fc = nn.Linear(hidden_dim, horizon)\n",
    "\n",
    "  def forward(self, x, edge_index):\n",
    "    x = x.unsqueeze(-1)\n",
    "\n",
    "    out, _ = self.gru(x)\n",
    "    out = out[:, -1, :]\n",
    "\n",
    "    out = F.relu(self.gc1(out, edge_index))\n",
    "    out = F.relu(self.gc2(out, edge_index))\n",
    "\n",
    "    out = self.fc(out)\n",
    "    return out\n",
    "\n",
    "window_size = 30\n",
    "hidden_dim = 32\n",
    "horizon = 2\n",
    "lr = 0.001\n",
    "epochs = 50\n",
    "batch_size = 32\n",
    "\n",
    "X_train = torch.stack([torch.tensor(pw, dtype=torch.float) for pw, _ in train_set])\n",
    "y_train = torch.stack([torch.tensor(fw, dtype=torch.float) for _, fw in train_set])\n",
    "\n",
    "X_val = torch.stack([torch.tensor(pw, dtype=torch.float) for pw, _ in val_set])\n",
    "y_val = torch.stack([torch.tensor(fw, dtype=torch.float) for _, fw in val_set])\n",
    "\n",
    "X_test = torch.stack([torch.tensor(pw, dtype=torch.float) for pw, _ in test_set])\n",
    "y_test = torch.stack([torch.tensor(fw, dtype=torch.float) for _, fw in test_set])\n",
    "\n",
    "train_dataset = TensorDataset(X_train, y_train)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "val_dataset = TensorDataset(X_val, y_val)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "test_dataset = TensorDataset(X_test, y_test)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "model = STGNN(window_size, hidden_dim, horizon)\n",
    "loss_fn = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    total_loss = 0\n",
    "    for x_batch, y_batch in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        preds = torch.stack([model(x, graph_edges) for x in x_batch])\n",
    "        loss = loss_fn(preds, y_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    avg_train_loss = total_loss / len(train_loader)\n",
    "\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for x_batch, y_batch in val_loader:\n",
    "            preds = torch.stack([model(x, graph_edges) for x in x_batch])\n",
    "            loss = loss_fn(preds, y_batch)\n",
    "            val_loss += loss.item()\n",
    "    avg_val_loss = val_loss / len(val_loader)\n",
    "    \n",
    "    print(f\"Epoch {epoch+1}/{epochs}, Loss: {total_loss/len(train_loader):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55109245",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = torch.stack([torch.tensor(pw, dtype=torch.float) for pw, _ in test_set])\n",
    "y_test = torch.stack([torch.tensor(fw, dtype=torch.float) for _, fw in test_set])\n",
    "\n",
    "test_dataset = TensorDataset(X_test, y_test)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "model.eval()\n",
    "test_loss = 0\n",
    "with torch.no_grad():\n",
    "    for x_batch, y_batch in test_loader:\n",
    "        preds = torch.stack([model(x, graph_edges) for x in x_batch])\n",
    "        loss = loss_fn(preds, y_batch)\n",
    "        test_loss += loss.item()\n",
    "test_loss /= len(test_loader)\n",
    "\n",
    "print(test_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39c5d919",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model, '/content/drive/MyDrive/Colab Notebooks/CivSentinel/STGNN_Model')\n",
    "torch.save(graph_edges, '/content/drive/MyDrive/Colab Notebooks/CivSentinel/STGNN_Model_Graph')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
